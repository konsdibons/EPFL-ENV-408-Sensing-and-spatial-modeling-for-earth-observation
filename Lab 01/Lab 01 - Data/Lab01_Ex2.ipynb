{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSMEO Lab 01 : Image coordinates, distortion model\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Coordinate projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages \n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.optimize import fsolve\n",
    "from parse_xml import parse_xml\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set path to the data folder downloaded on moodle\n",
    "path = ''\n",
    "\n",
    "img_id = 1092311568\n",
    "\n",
    "im_path = path + f'{img_id}_marked.jpg'\n",
    "\n",
    "img = cv2.imread(im_path)\n",
    "\n",
    "cam_IO_path = path + f'cam_param.txt'\n",
    "\n",
    "out_path = 'path to save your outputs'\n",
    "\n",
    "# Your code : load camera IO coefficients from cam_param.txt file and assign to variables (h, w, c, cx, cy, k1, k2, p1, p2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_path = f'path_to/YOUR_FILE_NAME_HERE.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Agisoft manual measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the parse_xml function to load and parse your data in a np.array with ids and a second np.array with uv coordinates\n",
    "id, uv_raw = parse_xml(marker_path)\n",
    "\n",
    "print(f\"Loaded {uv_raw.shape[0]} measurements :\")\n",
    "for i in range(uv_raw.shape[0]):\n",
    "    print(f\"Id : {id[i]}, u : {uv_raw[i,0]}, v : {uv_raw[i,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Definition of conversion between image coordinate systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, measurements were expressed in \"Top Left\" image coordinates, expressed in pixels :\n",
    "    <ul>\n",
    "    <li>The origin is the top-left corner of the image</li>\n",
    "    <li>U is oriented horizontally →</li>\n",
    "    <li>V is oriented downward ↓</li>\n",
    "    </ul> \n",
    "    \n",
    "In the next part of this lab, it is necessary to express measurements in perspective centered coordinates, express with unitless coordinates :\n",
    "    <ul>\n",
    "    <li>The origin is the perspective center of the image, the center of the pixels array shifted along x and y axis by the principal point offset in pixels $c_x, c_y$ </li>\n",
    "    <li>X is oriented horizontally →</li>\n",
    "    <li>Y is oriented downward ↓</li>\n",
    "    </ul> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given : \n",
    "    <ul>\n",
    "    <li>$(u,v)$ : coordinates in top-left image coordinate [px]</li>\n",
    "    <li>$(x,y)$ : coordinates in perspective-centered coordinates [-]</li>\n",
    "    <li>$c_x, c_y$ : perspective center offset [px]</li>\n",
    "    <li>$w, h$ : image width, height [px]</li>\n",
    "    <li>$c$ : focal length [px]</li>\n",
    "    </ul> \n",
    "    \n",
    "\\begin{equation*}\n",
    "x = \\frac{u - (\\frac{w-1}{2} + c_x)}{c}\n",
    "\\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "y = \\frac{v - (\\frac{h-1}{2} + c_y)}{c}\n",
    "\\tag{2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Note that to convert to perspective sensor coordinates, one aspect of the camera intrinsic orientation is considered : the center offset.<br>\n",
    "\n",
    "This offset is generally due to the imperfect mounting of the lens with respect to the sensor, causing the center of the lens to be slightly shifted with respect to the center of the sensor.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Define function **topleft2perspective** to project pixel coordinates from sensor to perspective centered frame\n",
    "\n",
    "(b) Define function **perspective2topleft** to project pixel coordinates from perspective to sensor centered frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topleft2perspective(uv, h, w, cx, cy, c):\n",
    "    '''\n",
    "    Project from top left to perspective image coordinate \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    uv : np.array(Nx2), the array of measurements to project\n",
    "        \n",
    "    h, w : image height and width\n",
    "         \n",
    "    cx, cy, c : corresponding camera calibration parameters\n",
    "            \n",
    "    Return\n",
    "    ------\n",
    "    xy : np.array(Nx2), the reprojected array \n",
    "    '''\n",
    "    # Your code : convert coordinates\n",
    "\n",
    "    return xy\n",
    "\n",
    "#Define function to project from sensor to perspective coordinate system\n",
    "def perspective2topleft(xy, h, w, cx, cy, c):\n",
    "    '''\n",
    "    Project from perspective to top-left image coordinate \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    xy : np.array(Nx2), the array of measurements to project\n",
    "\n",
    "    h, w : image height and width\n",
    "\n",
    "    cx, cy, c : corresponding camera calibration parameters\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    uv : np.array(Nx2), the reprojected array    \n",
    "    '''\n",
    "    # Your code : convert coordinates\n",
    "    \n",
    "    return uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test that you reprojections are correct :\n",
    "topleft_dummy = np.array([[0,0],\n",
    "                          [(w-1)/2+cx, (h-1)/2+cy]])\n",
    "\n",
    "perspective_dummy = np.array(([[0,0],\n",
    "                               [(-(w-1)/2-cx)/c, (-(h-1)/2-cy)/c]]))\n",
    "\n",
    "#Test the results of your conversion function\n",
    "assert(np.isclose(topleft2perspective(topleft_dummy, h, w, cx, cy, c),\n",
    "                  [[(-(w-1)/2-cx)/c, (-(h-1)/2-cy)/c],[0, 0]],\n",
    "                  atol = 1e-3).all())\n",
    "assert(np.isclose(perspective2topleft(perspective_dummy, h, w, cx, cy, c),\n",
    "                  [[((w-1)/2+cx), (h-1)/2+cy],[0, 0]],\n",
    "                 atol = 1e-3).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Project and save measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert uv (top left) to xy (perspective)\n",
    "xy_raw = \n",
    "\n",
    "print(f\"Measurements in perspective coordinates :\")\n",
    "for i in range(xy_raw.shape[0]):\n",
    "    print(f\"Id = {id[i]} : x = {xy_raw[i,0]:.8f}, y = {xy_raw[i,1]:.8f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Save your measurements along with ids. \n",
    " \n",
    "Appart from the **id_xy.txt** and **id_uv.txt** measurements file, screenshots of each of your measurements will be exported to the **/measurements** folder.  \n",
    "\n",
    "You can compare the close range screenshots of your measurements with the provided one (folder raw_data/GCPs_1092311568/) to assess the precision of your marking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate id with to both uv and xy\n",
    "id_uv_raw = np.concatenate((np.array(id).astype(int).reshape(-1,1),uv_raw),axis=1)\n",
    "id_xy_raw = np.concatenate((np.array(id).astype(int).reshape(-1,1),xy_raw),axis=1)\n",
    "\n",
    "# Save those into two separate files\n",
    "np.savetxt(out_path + f'{img_id}_id_uv_raw.txt',\n",
    "           id_uv_raw, fmt='%f',\n",
    "           header = 'id; u; v',\n",
    "           delimiter = ';')\n",
    "\n",
    "np.savetxt(out_path + f'{img_id}_id_xy_raw.txt',\n",
    "           id_xy_raw, fmt='%f',\n",
    "           header = 'id; x; y',\n",
    "           delimiter = ';')\n",
    "\n",
    "print(f\"Saving {xy_raw.shape[0]} measurements to {out_path}{img_id}_id_xy_raw.txt and {out_path}{img_id}_id_uv_raw.txt :\")\n",
    "\n",
    "for i in range(uv_raw.shape[0]):\n",
    "    \n",
    "    uv_raw = uv_raw.astype(int)\n",
    "    blue = (200,200,40)\n",
    "    orn = (0,120,255)\n",
    "    \n",
    "    cv2.line(img, (uv_raw[i,0], uv_raw[i,1]-25), (uv_raw[i,0], uv_raw[i,1]+25), blue, 1)\n",
    "    cv2.line(img, (uv_raw[i,0]-25, uv_raw[i,1]), (uv_raw[i,0]+25, uv_raw[i,1]), blue, 1)\n",
    "    \n",
    "    cv2.rectangle(img, (uv_raw[i,0]-350, uv_raw[i,1]-350), (uv_raw[i,0]+350, uv_raw[i,1]+350), blue, 50)\n",
    "    cv2.rectangle(img, (uv_raw[i][0]-50, uv_raw[i][1]-50), (uv_raw[i][0]+50, uv_raw[i][1]+50), orn, 5)\n",
    "    cv2.putText(img, str(id[i]), (uv_raw[i,0]-60, uv_raw[i,1]-65), cv2.FONT_HERSHEY_SIMPLEX, 1, orn, 4, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imwrite(out_path + f\"Manual_GCP_{id[i]}_{img_id}.jpg\",\n",
    "                img[uv_raw[i,1]-350:uv_raw[i,1]+350,uv_raw[i,0]-350:uv_raw[i,0]+350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Undistort manual measurements\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Objectives:</b> Cameras are not perfect and distortion due to imperfect mounting and lens distortion must be corrected to accurately map each pixel. \n",
    "    \n",
    "In this section you will implement a simple distortion model and visualize its effect on your measurements\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Set up\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load raw measurements you did in Part 1.:\n",
    "\n",
    "id_xy_raw = np.loadtxt(path +f'measurements/{img_id}_id_xy_raw.txt', delimiter=';')\n",
    "print(f\"Loaded {id_xy_raw.shape[0]} measurements :\")\n",
    "print(id_xy_raw)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define camera distortion model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the simplified Contrady-Brown distortion model. \n",
    "This model relates distorded image coordinates $x', y'$ (the points you just measured), to the undistorded ones $x, y$ through radial coefs ($k_1, k_2$) and tangential coefs ($p_1, p_2$) :\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "x(1 + k_1\\cdot r² + k_2 \\cdot r⁴) + p_1(r² + 2\\cdot x²) + 2\\cdot p_2\\cdot x\\cdot y - x' = 0\n",
    "\\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "y(1 + k_1\\cdot r² + k_2\\cdot r⁴) + p_2(r² + 2\\cdot y²) + 2\\cdot p_1\\cdot x\\cdot y - y' = 0\n",
    "\\tag{2}\n",
    "\\end{equation*}\n",
    "\n",
    "With $r² = x² + y²$      \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Define model\n",
    "\n",
    "We will use scipy non-linear solver to find for any measurment on the distorded image ($x', y'$) the corresponding undistorded measurement ($x, y$).\n",
    "\n",
    "Define **undisort** function, which returns the two symbolic equations (1) and (2), given model's parameters, and undistorded coordinate of a measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undistort(var,x_d,y_d,k1,k2,p1,p2):  \n",
    "    '''\n",
    "    Define equation (1) and (2) given a complet set of parameter and distorded measurement coordinates\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    var : tupple containing x and y variables. This are the two variables scipy solver will solve for\n",
    "        \n",
    "    x_d, y_d : floats, the coordinates of a measurment on the distorded image\n",
    "        \n",
    "    k1, k2, p1, p2 : floats, Contrady-Brown distortion model coefficients\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    \n",
    "    list of size 2 containing both eqation of the Contrady-Brown model\n",
    "    '''\n",
    "    #Unpack variables\n",
    "    x, y = var\n",
    "    #Your code : 2 elements list containing equation (1) and (2) depending on x,y\n",
    "\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at [scipy.optimize.fsolve documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fsolve.html) to understand how to format the input.  \n",
    "Initial guess can be initialized with the raw measurements. Hence your raw measurements will be called twice, once packed in **var** tupple, as initial guess for the fsolve to start optimizing, and once in x_d and y_d variables, representing the raw coordinate in the **undistort** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code : call the fsolve method on the undistort function to solve for x and y at each measurement\n",
    "\n",
    "\n",
    "# Your code : store into xy_corrected Nx2 np.array\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Visualize the effect of the distortion on your measurements and auto evaluate your implementation\n",
    "\n",
    "The **plot_measurements** function provided below will do this for you (no need to modify it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_measurements(img, raw_pts, undist_pts):\n",
    "    raw_pts_i = raw_pts.astype(int)\n",
    "    undist_pts_i = undist_pts.astype(int)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    r = (255,0,0)\n",
    "    g = (0,255,0)\n",
    "    for i in range(len(raw_pts)):\n",
    "        cv2.line(img, (raw_pts_i[i]+[-15, 0]), (raw_pts_i[i]+[ 15, 0]), r, 1)\n",
    "        cv2.line(img, (raw_pts_i[i]+[0, -15]), (raw_pts_i[i]+[0,  15]), r, 1)\n",
    "        cv2.line(img, (undist_pts_i[i]+[-15, 0]), (undist_pts_i[i]+[ 15, 0]), g, 1)\n",
    "        cv2.line(img, (undist_pts_i[i]+[0, -15]), (undist_pts_i[i]+[0,  15]), g, 1)\n",
    "        plt.imshow(img[raw_pts_i[i,1]-150:raw_pts_i[i,1]+150,\n",
    "                        raw_pts_i[i,0]-150:raw_pts_i[i,0]+150])\n",
    "        plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image provided contains indication for you to evaluate your measurements and their distortion.  \n",
    "For each GCP, plot_measurement will display a zoom around it. \n",
    "\n",
    "On each image, the red square represents the area where your raw measurement should land. The red cross represents the actual position of your raw manual measurement.\n",
    "If your measurement does not fall into the square, GCP manual measurements or coordinate conversion might be incorrect.\n",
    "\n",
    "The green square represents the area where your undistorted measurement should land. The green cross represents the actual position of your undistorted measurement.\n",
    "If your measurement does not fall into the square, you  mast likely have an issue with the distortion part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code : project your corrections to top left coordinates and display \n",
    "uv_corrected =\n",
    "\n",
    "#Load the original measurements in top left coordinates\n",
    "uv_raw = np.loadtxt(out_path + f'{img_id}_id_uv_raw.txt', delimiter=';')[:,1:]\n",
    "\n",
    "im_path = f'raw_data/{img_id}_assessment.jpg'\n",
    "img = cv2.imread(im_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_measurements(img, uv_raw, uv_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code : concatenate measurements ids, u and v into a Nx3 np.array\n",
    "# and save it to a txt file, same format as part 1\n",
    "id_uv_corrected =\n",
    "\n",
    "np.savetxt(out_path + f'{img_id}_id_uv_corrected.txt',\n",
    "           id_uv_corrected,\n",
    "           fmt='%f',\n",
    "           header = 'id; u corrected; v corrected',\n",
    "           delimiter = ';')\n",
    "\n",
    "print('Measurements displacement from undistortion [px]: ')\n",
    "print(uv_raw-uv_corrected)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : Visualize the camera distortion model \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Objectives:</b> In this section, we aim at understanding the effect of the different distortion coefficient on the image correction.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **grid_points** generates a regular square grid on the image we spacing of **k** pixels. \n",
    "\n",
    "Use it to generate a grid every 100 pixel and undistort those points using the same function as in **Part 2**.\n",
    "\n",
    "Try using different distortion coefficients to understand the influence of each one of them on the total distortion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_points(img, k):\n",
    "    '''\n",
    "    Generates a regular grid of points from an image in top left coordinates\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : cv2.image\n",
    "        the image from which take a grid\n",
    "    k : int\n",
    "        the grid spacing in pixel\n",
    "            \n",
    "    Return\n",
    "    ------\n",
    "    grid : Nx2 np.array\n",
    "        the array of the grid, each line contains x,y coordinate of a given node\n",
    "    '''\n",
    "    height, width, channels = img.shape\n",
    "    \n",
    "    x, y = np.meshgrid(np.arange(0,width, k),\n",
    "                       np.arange(0,height, k))\n",
    "    return np.array([x.flatten(), y.flatten()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code : Generate the grid with one point every 100 pixels and project into perspective centered coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define arbitrary distortion coefficient to oberve the effect of each component on the distortion\n",
    "k1_dummy = 0 # 0.1\n",
    "k2_dummy = 0 # 0.1\n",
    "p1_dummy = 0 # 0.05\n",
    "p2_dummy = 0.05 # 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code : Apply distortion model to the grid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the result of the undistorded coordinates estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distortions(img, raw_pts, undist_pts, thickness):\n",
    "    dist = np.linalg.norm(raw_pts - undist_pts, axis=1)\n",
    "    min_d = np.min(dist)\n",
    "    max_d = np.max(dist)\n",
    "\n",
    "    raw_pts_i = raw_pts.astype(int)\n",
    "    undist_pts_i = undist_pts.astype(int)\n",
    "\n",
    "    for i in range(len(raw_pts)):\n",
    "        color = (0,\n",
    "                 255*(1 -(dist[i]-min_d)/(max_d-min_d))**2,\n",
    "                 255*((dist[i]-min_d)/(max_d-min_d))**0.5)\n",
    "        \n",
    "        cv2.line(img, (raw_pts_i[i]), (undist_pts_i[i]), color, thickness)\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load image & plot\n",
    "im_path = f'raw_data/{img_id}.jpg'\n",
    "img = cv2.imread(im_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code : project both grids (raw and corrected) to sensor centered coordinates and visualize the results\n",
    "# Use a thickness of 15 to easily observe distortion rays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_uv-grid_uv_corrected)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c4453615cd91750b42a92082c1d2e45800de486616e18c6f6e61d58c8451ca01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
